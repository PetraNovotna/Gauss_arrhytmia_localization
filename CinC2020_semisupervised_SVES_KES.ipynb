{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CinC2020_semisupervised_SVES_KES.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PetraNovotna/semisupervised_KES_SVES/blob/master/CinC2020_semisupervised_SVES_KES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDKbOOqQ6wCR",
        "colab_type": "text"
      },
      "source": [
        "Prvni textova bunecka."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cI_ELCE4FeT",
        "colab_type": "code",
        "outputId": "89835fce-abb5-4e94-c183-4814e09ac0ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Hello, world!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello, world!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwp6EXsa7Und",
        "colab_type": "code",
        "outputId": "b0fac6eb-d80b-40da-f2db-7e24d41820d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#### mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by9ZuKarE1B5",
        "colab_type": "text"
      },
      "source": [
        "Stazeni dat z repo na challenge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGusYAZH7iHH",
        "colab_type": "code",
        "outputId": "123270c3-59c6-4c7f-8bde-f8647fee6a01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!wget -O PhysioNetChallenge2020_Training_CPSC.tar.gz https://cloudypipeline.com:9555/api/download/physionet2020training/PhysioNetChallenge2020_Training_CPSC.tar.gz/ -P /content/drive/My Drive/CinC2020_semisupervised_SVES_KES"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-22 08:30:55--  https://cloudypipeline.com:9555/api/download/physionet2020training/PhysioNetChallenge2020_Training_CPSC.tar.gz/\n",
            "Resolving cloudypipeline.com (cloudypipeline.com)... 34.74.104.185\n",
            "Connecting to cloudypipeline.com (cloudypipeline.com)|34.74.104.185|:9555... connected.\n",
            "HTTP request sent, awaiting response... 200 \n",
            "Length: 859926186 (820M) [application/octet-stream]\n",
            "Saving to: ‘PhysioNetChallenge2020_Training_CPSC.tar.gz’\n",
            "\n",
            "2020_Training_CPSC. 100%[===================>] 820.09M  15.5MB/s    in 55s     \n",
            "\n",
            "2020-04-22 08:31:51 (14.9 MB/s) - ‘PhysioNetChallenge2020_Training_CPSC.tar.gz’ saved [859926186/859926186]\n",
            "\n",
            "--2020-04-22 08:31:51--  http://drive/CinC2020_semisupervised_SVES_KES\n",
            "Resolving drive (drive)... failed: No address associated with hostname.\n",
            "wget: unable to resolve host address ‘drive’\n",
            "FINISHED --2020-04-22 08:31:51--\n",
            "Total wall clock time: 56s\n",
            "Downloaded: 1 files, 820M in 55s (14.9 MB/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm1LH5DLErGS",
        "colab_type": "text"
      },
      "source": [
        "Rozbaleni dat "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbcpSGMKCgYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xvzf '/content/drive/My Drive/CinC2020_semisupervised_SVES_KES/PhysioNetChallenge2020_Training_CPSC.tar.gz' -C '/content/drive/My Drive/CinC2020_semisupervised_SVES_KES/data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9thbGBgYGF2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import scipy.io as io\n",
        "\n",
        "\n",
        "def read_data(path, file_name):\n",
        "    data_dict = io.loadmat(os.path.join(path, file_name))\n",
        "    return data_dict[\"val\"]\n",
        "\n",
        "def read_lbl(path, file_name):\n",
        "    name=os.path.join(path, file_name)+\".hea\"\n",
        "\n",
        "    # Read line 15 in header file and parse string with labels\n",
        "    with open(name, \"r\") as file:\n",
        "        for line_idx, line in enumerate(file):\n",
        "            if line_idx == 15:\n",
        "                line=line.replace('#Dx: ','')\n",
        "                lbl=line\n",
        "                break\n",
        "    file.close()\n",
        "    return lbl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjKEXZ9gGuor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config:\n",
        "    \n",
        "    \n",
        "    best_models_dir=\"/content/drive/My Drive/CinC2020_semisupervised_SVES_KES/models\"\n",
        "    \n",
        "   # PVC  vs  ostatní data bez LBBB, RBBB, PAC\n",
        "\n",
        "   # PVC  vs  normal\n",
        "\n",
        "    # pato_names=['Normal','AF','I-AVB','LBBB','RBBB','PAC','PVC','STD','STE']\n",
        "\n",
        "    pato_names=['Normal','PVC']\n",
        "    \n",
        "    DATA_PATH = \"/content/drive/My Drive/CinC2020_semisupervised_SVES_KES/data/Training_WFDB\"\n",
        "    \n",
        "\n",
        "    train_batch_size = 32\n",
        "    train_num_workers=4\n",
        "    valid_batch_size = 32\n",
        "    valid_num_workers=4\n",
        "\n",
        "    max_epochs = 107\n",
        "    step_size=35\n",
        "    gamma=0.1\n",
        "    init_lr=0.01\n",
        "\n",
        "    split_ratio=[8,2]\n",
        "\n",
        "\n",
        "    model_save_dir=\"/content/drive/My Drive/CinC2020_semisupervised_SVES_KES/tmp\"\n",
        "    \n",
        "    model_note='test1'\n",
        "\n",
        "    DATA_TMP_PATH= \"/content/drive/My Drive/CinC2020_semisupervised_SVES_KES/data/Training_WFDB_filtered\"\n",
        "\n",
        "    info_save_dir=\"/content/drive/My Drive/CinC2020_semisupervised_SVES_KES/tmp_info\"\n",
        "\n",
        "    \n",
        "    ## network setting\n",
        "    levels=4\n",
        "    lvl1_size=4\n",
        "    input_size=12\n",
        "    output_size=2\n",
        "    convs_in_layer=2\n",
        "    init_conv=4\n",
        "    filter_size=5\n",
        "    \n",
        "    \n",
        "    ploting=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S48y9wOGKIsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from shutil import copyfile\n",
        "import os\n",
        "\n",
        "try:\n",
        "    os.mkdir(Config.DATA_TMP_PATH)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "\n",
        "##get all file names\n",
        "names=[]\n",
        "for root, dirs, files in os.walk(Config.DATA_PATH):\n",
        "    for name in files:\n",
        "        if name.endswith(\".mat\"):\n",
        "            name=name.replace('.mat','')\n",
        "            names.append(name)\n",
        "\n",
        "\n",
        "for k,file_name in enumerate(names):\n",
        "\n",
        "    lbl = read_lbl(Config.DATA_PATH, file_name)\n",
        "    lbl = lbl[:-1]\n",
        "\n",
        "    for pato_name in Config.pato_names:\n",
        "      if pato_name==lbl:\n",
        "        copyfile(Config.DATA_PATH + os.sep +file_name +'.mat',Config.DATA_TMP_PATH + os.sep +file_name +'.mat')\n",
        "        copyfile(Config.DATA_PATH + os.sep +file_name + '.hea' ,Config.DATA_TMP_PATH + os.sep +file_name + '.hea')\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSkcN1CLG56C",
        "colab_type": "code",
        "outputId": "d0a19db2-8229-4b91-9827-bd3d0ae05f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "try:\n",
        "    os.mkdir(Config.info_save_dir)\n",
        "except:\n",
        "    pass\n",
        "    \n",
        "    \n",
        "##get all file names\n",
        "names=[]\n",
        "for root, dirs, files in os.walk(Config.DATA_TMP_PATH):\n",
        "    for name in files:\n",
        "        if name.endswith(\".mat\"):\n",
        "            name=name.replace('.mat','')\n",
        "            names.append(name)\n",
        "    \n",
        "    \n",
        "## measure signal statistics for normalization\n",
        "labels=[]\n",
        "means=[]\n",
        "stds=[]\n",
        "lens=[]\n",
        "for k,file_name in enumerate(names):\n",
        "\n",
        "    \n",
        "    X = read_data(Config.DATA_TMP_PATH, file_name)\n",
        "    \n",
        "    means.append(np.mean(X,axis=1))\n",
        "    stds.append(np.std(X,axis=1))\n",
        "    lens.append(X.shape[1])\n",
        "    \n",
        "    \n",
        "    lbl = read_lbl(Config.DATA_TMP_PATH, file_name)\n",
        "    \n",
        "    \n",
        "    labels.append(lbl)\n",
        "    \n",
        "MEANS=np.mean(np.stack(means,axis=1),axis=1)\n",
        "STDS=np.mean(np.stack(stds,axis=1),axis=1)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "## create more-hot-encoding to measure count of each pathology in data\n",
        "more_hot_lbls=[]\n",
        "for k,lbl in enumerate(labels):\n",
        "        \n",
        "    res=np.zeros(len(Config.pato_names))\n",
        "    \n",
        "    lbl=lbl.split(',')\n",
        "\n",
        "    for kk,p in enumerate(Config.pato_names):\n",
        "        for lbl_i in lbl:\n",
        "            if lbl_i.find(p)>-1:\n",
        "                res[kk]=1\n",
        "            \n",
        "    more_hot_lbls.append(res>0)\n",
        "\n",
        "tmp=np.stack(more_hot_lbls,axis=1)\n",
        "\n",
        "lbl_counts=np.sum(tmp,axis=1)\n",
        "    \n",
        "    \n",
        "    \n",
        "num_of_sigs=len(lens)\n",
        "\n",
        "\n",
        "\n",
        "print(MEANS)\n",
        "\n",
        "\n",
        "print(STDS)\n",
        "\n",
        "\n",
        "print(lbl_counts)\n",
        "\n",
        "\n",
        "print(len(lens))\n",
        "\n",
        "\n",
        "## save statistics\n",
        "np.save(Config.info_save_dir+os.sep+'MEANS.npy', np.array(MEANS))\n",
        "np.save(Config.info_save_dir+os.sep+'STDS.npy', np.array(STDS))\n",
        "np.save(Config.info_save_dir+os.sep+'lbl_counts.npy', np.array(lbl_counts))\n",
        "np.save(Config.info_save_dir+os.sep+'lens.npy', np.array(lens))\n",
        "    \n",
        "np.random.seed(666)\n",
        "\n",
        "split_ratio_ind=int(np.floor(Config.split_ratio[0]/(Config.split_ratio[0]+Config.split_ratio[1])*len(names)))\n",
        "perm=np.random.permutation(len(names))\n",
        "train_ind=perm[:split_ratio_ind]\n",
        "valid_ind=perm[split_ratio_ind:]\n",
        "split= {'train': [names[i] for i in train_ind],'valid': [names[i] for i in valid_ind]}\n",
        "\n",
        "\n",
        "\n",
        "np.save(Config.info_save_dir+os.sep+'split.npy', split)\n",
        "\n",
        "np.save(Config.info_save_dir+os.sep+'num_of_sigs.npy', num_of_sigs)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.00679038 -0.00100342  0.00804317 -0.01208988  0.00664558 -0.00079111\n",
            " -0.00623427 -0.00737927 -0.00218951 -0.01022033  0.0002853  -0.00121358]\n",
            "[121.19576399 172.44484679 140.31936859 134.0900503  102.17357943\n",
            " 144.50577865 194.05515306 299.74809719 283.92871109 291.64460943\n",
            " 271.84420298 238.5313647 ]\n",
            "[918 607]\n",
            "1525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frOL8KPSTxsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_siYsImpGKzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import torch \n",
        "\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset generator class\n",
        "    Ref: https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, list_of_ids, data_path,split):\n",
        "        \"\"\"Initialization\"\"\"\n",
        "        self.path = data_path\n",
        "        self.list_of_ids = list_of_ids\n",
        "        self.split=split\n",
        "        \n",
        "        self.MEANS=np.load('data_split/MEANS.npy')\n",
        "    \n",
        "        self.STDS=np.load('data_split/STDS.npy')\n",
        "        \n",
        "        self.pato_names=np.load('data_split/pato_names.npy')\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return total number of data samples\"\"\"\n",
        "        return len(self.list_of_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Generate data sample\"\"\"\n",
        "        # Select sample\n",
        "        file_name = self.list_of_ids[idx]\n",
        "\n",
        "        # Read data and get label\n",
        "        X = read_data(self.path, file_name)\n",
        "        \n",
        "        \n",
        "        \n",
        "        sig_len=X.shape[1]\n",
        "        signal_num=X.shape[0]\n",
        "        \n",
        "        ##augmentation\n",
        "        if self.split=='train':\n",
        "            ##random circshift\n",
        "            if torch.rand(1).numpy()[0]>0.3:\n",
        "                \n",
        "                \n",
        "                shift=torch.randint(sig_len,(1,1)).view(-1).numpy()\n",
        "                \n",
        "                X=np.roll(X, shift, axis=1)\n",
        "                \n",
        "            ## random stretch -    \n",
        "            if torch.rand(1).numpy()[0]>0.3:\n",
        "                \n",
        "                max_resize_change=0.2\n",
        "                relative_change=1+torch.rand(1).numpy()[0]*2*max_resize_change-max_resize_change\n",
        "                new_len=int(relative_change*sig_len)\n",
        "                \n",
        "                Y=np.zeros((signal_num,new_len))\n",
        "                for k in range(signal_num):\n",
        "\n",
        "                    Y[k,:]=np.interp(np.linspace(0, sig_len-1, new_len),np.linspace(0, sig_len-1, sig_len),X[k,:])\n",
        "                X=Y\n",
        "                \n",
        "            ## random multiplication of each lead by a number\n",
        "            if torch.rand(1).numpy()[0]>0.3:\n",
        "                \n",
        "                max_mult_change=0.2\n",
        "                \n",
        "                for k in range(signal_num):\n",
        "                    mult_change=1+torch.rand(1).numpy()[0]*2*max_mult_change-max_mult_change\n",
        "                    X[k,:]=X[k,:]*mult_change\n",
        "                    \n",
        "                \n",
        "                \n",
        "        ## normalization\n",
        "        X=(X-self.MEANS.reshape(-1,1))/self.STDS.reshape(-1,1)\n",
        "        \n",
        "        \n",
        "        ## laod label\n",
        "        lbl = read_lbl(self.path, file_name)\n",
        "        lbl=lbl.split(',')\n",
        "        \n",
        "        ## create more hot encoding\n",
        "        y=np.zeros((len(self.pato_names),1)).astype(np.float32)\n",
        "        for kk,p in enumerate(self.pato_names):\n",
        "            for lbl_i in lbl:\n",
        "                if lbl_i.find(p)>-1:\n",
        "                    y[kk]=1\n",
        "\n",
        "        return X,y\n",
        "    \n",
        "    def collate_fn(data):\n",
        "        ## this take list of samples and put them into batch\n",
        "        \n",
        "        ##pad with zeros\n",
        "        pad_val=0\n",
        "        \n",
        "        ## get list of singals and its lengths\n",
        "        seqs, lbls = zip(*data)\n",
        "        \n",
        "        lens = [seq.shape[1] for seq in seqs]\n",
        "        \n",
        "        ## pad shorter signals with zeros to make them same length\n",
        "        padded_seqs =pad_val*np.ones((len(seqs),seqs[0].shape[0], np.max(lens))).astype(np.float32)\n",
        "        for i, seq in enumerate(seqs):\n",
        "            end = lens[i]\n",
        "            padded_seqs[i,:, :end] = seq\n",
        "        \n",
        "        \n",
        "        ## stack and reahape signal lengts to 10 vector\n",
        "        lbls=np.stack(lbls,axis=0)\n",
        "        lbls=lbls.reshape(lbls.shape[0:2])\n",
        "        lens = np.array(lens).astype(np.float32)\n",
        "        \n",
        "        ## numpy -> torch tensor\n",
        "        padded_seqs=torch.from_numpy(padded_seqs)\n",
        "        lbls=torch.from_numpy(lbls)\n",
        "        lens=torch.from_numpy(lens)\n",
        "        \n",
        "        return padded_seqs,lens,lbls\n",
        "        \n",
        "\n",
        "\n",
        "def main():\n",
        "    return Dataset\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZMY33GNGX2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from config import Config\n",
        "\n",
        "class Log:\n",
        "    def __init__(self):\n",
        "        self.t=None\n",
        "        \n",
        "        self.model_names=[]\n",
        "        \n",
        "        self.trainig_loss_log=[]\n",
        "        self.trainig_beta_log=[]\n",
        "        self.valid_loss_log=[]\n",
        "        self.valid_beta_log=[]\n",
        "        \n",
        "        self.lbls_np_log=[]\n",
        "        self.res_np_log=[]\n",
        "        self.tmp_loss_log=[]\n",
        "        \n",
        "    def save_tmp_log(self,lbls,res,loss):\n",
        "        ## save values - epoch mean will be calucated later\n",
        "        self.lbls_np_log.append(lbls.detach().cpu().numpy())\n",
        "        self.res_np_log.append(res.detach().cpu().numpy())\n",
        "        self.tmp_loss_log.append(loss.detach().cpu().numpy())\n",
        "    \n",
        "    def save_log_data_and_clear_tmp(self,train_or_test):\n",
        "\n",
        "        lbls_np_log=np.concatenate(self.lbls_np_log,axis=0)\n",
        "        res_np_log=np.concatenate(self.res_np_log,axis=0)\n",
        "        if Config.best_t:\n",
        "            ## find optimal treshold for each pathology\n",
        "            self.t=get_best_ts(res_np_log,lbls_np_log)\n",
        "        else:\n",
        "            self.t=0.5\n",
        "        ## get beta score - challage metric\n",
        "        Fbeta,Gbeta,geom_mean= compute_beta_score(lbls_np_log, res_np_log>self.t, 2, 9)\n",
        "        \n",
        "        ## save appchoch averages\n",
        "        if train_or_test=='train':\n",
        "            self.trainig_loss_log.append(np.mean(self.tmp_loss_log))\n",
        "            self.trainig_beta_log.append(geom_mean)\n",
        "        elif train_or_test=='valid':\n",
        "            self.valid_loss_log.append(np.mean(self.tmp_loss_log))\n",
        "            self.valid_beta_log.append(geom_mean)\n",
        "        else:\n",
        "            raise ValueError('train or test')\n",
        "    \n",
        "        ## crear tmp epoch data buffers\n",
        "        self.lbls_np_log=[]\n",
        "        self.res_np_log=[]\n",
        "        self.tmp_loss_log=[]\n",
        "        \n",
        "\n",
        "    def save_log_model_name(self,model_name):\n",
        "        ## store model names\n",
        "        self.model_names.append(model_name)\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "def get_best_ts(res_np_log,lbls_np_log):\n",
        "\n",
        "    res_np_log\n",
        "    t=np.zeros(np.shape(res_np_log))\n",
        "    for f_num in range(res_np_log.shape[1]):\n",
        "        \n",
        "        \n",
        "        f=res_np_log[:,[f_num]]\n",
        "        l=lbls_np_log[:,[f_num]]\n",
        "        \n",
        "        t_best=0.5\n",
        "        v_best=-1\n",
        "        for tt in np.concatenate( (np.linspace(0,0.1,100),np.linspace(0,1,100),np.linspace(0,0.1,100)),axis=0):\n",
        "            \n",
        "            Fbeta,Gbeta,v= compute_beta_score(l, f>tt, 2, 1)\n",
        "            if v>v_best:\n",
        "                v_best=v\n",
        "                t_best=tt\n",
        "            \n",
        "        \n",
        "        \n",
        "        t[:,f_num]=t_best\n",
        "\n",
        "    return t[[0],:]\n",
        "\n",
        "\n",
        "def compute_beta_score(labels, output, beta=2, num_classes=9):\n",
        "    \n",
        "    \n",
        "  \n",
        "    TP=(labels==1)&(output==1)\n",
        "    FP=(labels==0)&(output==1)\n",
        "    FN=(labels==1)&(output==0)\n",
        "    \n",
        "    num_labels=np.sum(labels,axis=1,keepdims =1)\n",
        "    if num_classes==1:\n",
        "        num_labels=1\n",
        "    \n",
        "    TP=TP/num_labels\n",
        "    FP=FP/num_labels\n",
        "    FN=FN/num_labels\n",
        "    \n",
        "\n",
        "    TP=np.sum(TP,axis=0)/num_labels\n",
        "    FP=np.sum(FP,axis=0)/num_labels\n",
        "    FN=np.sum(FN,axis=0)/num_labels\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "    Fbetas=(1+beta**2)*TP/((1+beta**2)*TP+FP+beta**2*FN)\n",
        "    \n",
        "    \n",
        "    Fbetas[((1+beta**2)*TP+FP+beta**2*FN)==0]=1\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    Fbeta=np.mean(Fbetas)\n",
        "    \n",
        "    \n",
        "    Fbeta=np.mean(Fbetas)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    Gbetas=TP/(TP+FP+beta*FN)\n",
        "    \n",
        "    \n",
        "    Gbetas[(TP+FP+beta*FN)==0]=1\n",
        "    \n",
        "    \n",
        "    \n",
        "    Gbeta=np.mean(Gbetas)\n",
        "    Gbeta=np.mean(Gbetas)\n",
        "    \n",
        "    geom_mean=np.sqrt(Gbeta*Fbeta)\n",
        "    \n",
        "    return Fbeta,Gbeta,geom_mean\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "    \n",
        "    \n",
        "    \n",
        "def wce(res,lbls,w_positive_tensor,w_negative_tensor):\n",
        "    ## weighted crossetropy - weigths are for positive and negative \n",
        "    res_c = torch.clamp(res,min=1e-6,max=1-1e-6)\n",
        "            \n",
        "    p1=lbls*torch.log(res_c)*w_positive_tensor\n",
        "    p2=(1-lbls)*torch.log(1-res_c)*w_negative_tensor\n",
        "    \n",
        "    return -torch.mean(p1+p2)\n",
        "    \n",
        "\n",
        "def beta_loss(output, labels,w_positive_tensor,w_negative_tensor,beta=2):\n",
        "    ## loss inspired by challage metric - binary values are replaced by s\n",
        "    smooth = 0.1\n",
        "\n",
        "    TP=(labels)*(output)\n",
        "    FP=(1-labels)*(output)\n",
        "    FN=(labels)*(1-output)\n",
        "    \n",
        "    \n",
        "    num_labels=torch.sum(labels,dim=1,keepdims =True)\n",
        "    \n",
        "    TP=TP/num_labels\n",
        "    FP=FP/num_labels\n",
        "    FN=FN/num_labels\n",
        "    \n",
        "\n",
        "    TP=torch.sum(TP,dim=0)/num_labels\n",
        "    FP=torch.sum(FP,dim=0)/num_labels\n",
        "    FN=torch.sum(FN,dim=0)/num_labels\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "    Fbetas=((1+beta**2)*TP+smooth)/((1+beta**2)*TP+FP+beta**2*FN+smooth)\n",
        "    \n",
        "    Gbetas=(TP+smooth)/(TP+FP+beta*FN+smooth)\n",
        "                        \n",
        "    Fbeta=torch.mean(Fbetas)\n",
        "    Gbeta=torch.mean(Gbetas)\n",
        "    \n",
        "    \n",
        "    return -torch.sqrt(Fbeta*Gbeta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqVtH3xoFwsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.nn import init\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class myConv(nn.Module):\n",
        "    def __init__(self, in_size, out_size,filter_size=3,stride=1,pad=None,do_batch=1,dov=0):\n",
        "        super().__init__()\n",
        "        \n",
        "        pad=int((filter_size-1)/2)\n",
        "        \n",
        "        self.do_batch=do_batch\n",
        "        self.dov=dov\n",
        "        self.conv=nn.Conv1d(in_size, out_size,filter_size,stride,pad)\n",
        "        self.bn=nn.BatchNorm1d(out_size,momentum=0.1)\n",
        "        \n",
        "        \n",
        "        if self.dov>0:\n",
        "            self.do=nn.Dropout(dov)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "     \n",
        "        outputs = self.conv(inputs)\n",
        "        if self.do_batch:\n",
        "            outputs = self.bn(outputs)  \n",
        "        \n",
        "        outputs=F.relu(outputs)\n",
        "        \n",
        "        if self.dov>0:\n",
        "            outputs = self.do(outputs)\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "\n",
        "        \n",
        "class Net_addition_grow(nn.Module):\n",
        "    def set_t(self,t):\n",
        "        self.t=t\n",
        "        \n",
        "    def get_t(self):\n",
        "        return self.t\n",
        "    \n",
        "    \n",
        "    def __init__(self, levels=7,lvl1_size=4,input_size=12,output_size=9,convs_in_layer=3,init_conv=4,filter_size=13,):\n",
        "        super().__init__()\n",
        "        self.levels=levels\n",
        "        self.lvl1_size=lvl1_size\n",
        "        self.input_size=input_size\n",
        "        self.output_size=output_size\n",
        "        self.convs_in_layer=convs_in_layer\n",
        "        self.filter_size=filter_size\n",
        "        \n",
        "        self.t=0.5*np.ones(output_size)\n",
        "        \n",
        "        \n",
        "        self.init_conv=myConv(input_size,init_conv,filter_size=filter_size)\n",
        "        \n",
        "        \n",
        "        self.layers=nn.ModuleList()\n",
        "        for lvl_num in range(self.levels):\n",
        "            \n",
        "            \n",
        "            if lvl_num==0:\n",
        "                self.layers.append(myConv(init_conv, int(lvl1_size*(lvl_num+1)),filter_size=filter_size))\n",
        "            else:\n",
        "                self.layers.append(myConv(int(lvl1_size*(lvl_num))+int(lvl1_size*(lvl_num))+init_conv, int(lvl1_size*(lvl_num+1)),filter_size=filter_size))\n",
        "            \n",
        "            for conv_num_in_lvl in range(self.convs_in_layer-1):\n",
        "                self.layers.append(myConv(int(lvl1_size*(lvl_num+1)), int(lvl1_size*(lvl_num+1)),filter_size=filter_size))\n",
        "\n",
        "\n",
        "        self.conv_final=myConv(int(lvl1_size*(self.levels))+int(lvl1_size*(self.levels))+init_conv, int(lvl1_size*self.levels),filter_size=filter_size)\n",
        "        \n",
        "        self.fc=nn.Linear(int(lvl1_size*self.levels), self.output_size)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        ## weigths initialization wih xavier method\n",
        "        for i, m in enumerate(self.modules()):\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.xavier_normal_(m.weight)\n",
        "                init.constant_(m.bias, 0)\n",
        "        \n",
        "        \n",
        "        \n",
        "    def forward(self, x,lens):\n",
        "        \n",
        "        \n",
        "        ## make signal len be divisible by 2**number of levels \n",
        "        ## replace rest by zeros\n",
        "        for signal_num in range(list(x.size())[0]):\n",
        "            \n",
        "            k=int(np.floor(lens[signal_num].cpu().numpy()/(2**(self.levels-1)))*(2**(self.levels-1)))\n",
        "            \n",
        "            x[signal_num,:,k:]=0\n",
        "        \n",
        "\n",
        "        ## pad with more zeros  -  add as many zeros as convolution of all layers can proppagete numbers\n",
        "        n=(self.filter_size-1)/2\n",
        "        padded_length=n\n",
        "        for p in range(self.levels):\n",
        "            for c in range(self.convs_in_layer):\n",
        "                padded_length=padded_length+2**p*n\n",
        "        padded_length=padded_length+2**p*n+256 # 256 for sure\n",
        "\n",
        "        \n",
        "        shape=list(x.size())\n",
        "        xx=torch.zeros((shape[0],shape[1],int(padded_length)),dtype=x.dtype)\n",
        "        cuda_check = x.is_cuda\n",
        "        if cuda_check:\n",
        "            cuda_device = x.get_device()\n",
        "            device = torch.device('cuda:' + str(cuda_device) )\n",
        "            xx=xx.to(device)\n",
        "        \n",
        "        x=torch.cat((x,xx),2)### add zeros to signal\n",
        "        \n",
        "        x.requires_grad=True\n",
        "        \n",
        "        \n",
        "        x=self.init_conv(x)\n",
        "        \n",
        "        x0=x\n",
        "        \n",
        "        ## aply all convolutions\n",
        "        layer_num=-1\n",
        "        for lvl_num in range(self.levels):\n",
        "            \n",
        "            \n",
        "            for conv_num_in_lvl in range(self.convs_in_layer):\n",
        "                layer_num+=1\n",
        "                if conv_num_in_lvl==1:\n",
        "                    y=x\n",
        "                \n",
        "                x=self.layers[layer_num](x)\n",
        "                \n",
        "            ## skip conection to previous layer and to the input\n",
        "            x=torch.cat((F.avg_pool1d(x0,2**lvl_num,2**lvl_num),x,y),1)\n",
        "            \n",
        "            x=F.max_pool1d(x, 2, 2)\n",
        "            \n",
        "            \n",
        "            \n",
        "        x=self.conv_final(x)\n",
        "        \n",
        "        ### replace padded parts of signals by -inf => it will be not used in poolig\n",
        "        for signal_num in range(list(x.size())[0]):\n",
        "            \n",
        "            k=int(np.floor(lens[signal_num].cpu().numpy()/(2**(self.levels-1))))\n",
        "            \n",
        "            x[signal_num,:,k:]=-np.Inf\n",
        "            \n",
        "        \n",
        "        \n",
        "        x=F.adaptive_max_pool1d(x,1)\n",
        "        \n",
        "        \n",
        "        # N,C,1\n",
        "        \n",
        "        x=x.view(list(x.size())[:2])\n",
        "        \n",
        "        # N,C\n",
        "        \n",
        "        x=self.fc(x)\n",
        "        \n",
        "        x=torch.sigmoid(x)\n",
        "        \n",
        "        return x   \n",
        "    \n",
        "    def save_log(self,log):\n",
        "        self.log=log\n",
        "        \n",
        "    def save_config(self,config):  \n",
        "        self.config=config\n",
        "        \n",
        "        \n",
        "    \n",
        "    def plot_training(self):\n",
        "        \n",
        "        plt.plot(self.log.trainig_loss_log,'b')\n",
        "        plt.plot(self.log.valid_loss_log,'r')\n",
        "        plt.title('loss')\n",
        "        plt.show()\n",
        "        \n",
        "        \n",
        "        plt.plot(self.log.trainig_beta_log,'b')\n",
        "        plt.plot(self.log.valid_beta_log,'g')\n",
        "        plt.title('geometric mean')\n",
        "        plt.show()\n",
        "    \n",
        "    def save_plot_training(self,name):\n",
        "        \n",
        "        plt.plot(self.log.trainig_loss_log,'b')\n",
        "        plt.plot(self.log.valid_loss_log,'r')\n",
        "        plt.title('loss')\n",
        "        plt.savefig(name + '_loss.png')\n",
        "        \n",
        "        \n",
        "        plt.plot(self.log.trainig_beta_log,'b')\n",
        "        plt.plot(self.log.valid_beta_log,'g')\n",
        "        plt.title('geometric mean')\n",
        "        plt.savefig(name + '_geometric_mean.png')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYUSNzoBF-T9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}